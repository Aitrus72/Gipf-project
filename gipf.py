	
####################################
#  David Sprehn and Espen Nielsen
#  March 2018
####################################

# This is a work in progress!
# We haven't yet completed training of an AI that can play well.

# Goal: we wanted to learn about neural networks and their AI applications,
#   so we decided to try training one to play the board game Gipf
# Neither of us knows how to play the game well -- an intentional choice
#   because we are really excited about the potential for this sort of 
#   technology to generate new knowledge, so we decided to avoid using any 
#   preexisting strategy heuristics or information about the game for
#   training the AI.  It will learn entirely from self-play


import numpy as np
from sklearn.neural_network import MLPRegressor
import random
import cProfile
import pickle
from joblib import Parallel, delayed
from time import sleep



###############################################################################
# AI player framework
###############################################################################

# setup for an AI game player, based on a neural network
# the network is trained to predict the "value" of a board position (1 for sure win, -1 for sure loss)
#   and can play by choosing moves with high value
# training data is generated by stochastically playing out many games from a given starting
#   position to estimate its value

# this framework (player and networkPlayer) could be directly applied to a different game
#    by creating a class 'board' with interface as described in the board class
#    and a function newGame() returning an instance that expresses the starting game position

# assumptions made about the game we are playing:
#   1) full games don't tend to last a very long time (say, <500 moves),
#      and the probability of an infinite game is zero (given decent play)
#   2) the number of possible moves in any position is bounded (<= 42 for Gipf)
#   3) there are two players, only one plays at a time (we don't necessarily assume alternation)
#      and the game is over when one cannot play, with a loss to that player
# if these conditions aren't met, then changes to the setup will be required.


# to try in the future:
#    a network which predicts both values and follow-up moves at the same time
#    try replacing Monte-Carlo playouts with Monte-Carlo tree search
#    try a residual network, other neural network architectures


# folder for storing generated training data and saved neural network coefficients
# modify this to fit your system:
baseFilename = '/home/insert-your-path-here/'    # (for linux)
#baseFilename = 'C:\\insert-your-path-here\\'    # (for windows)


class player(object):
	# represents an agent for predicting the values of board positions and for playing moves
	
	# not to be instanced directly
	# subclass with different backends (neural network regressor, random forest, etc.)
	# subclasses should implement eval and playMove, as follows:
	
	# self.eval(b) should return our estimate of the value of position b, from the
	#   point of view of the current player (1=win, -1=loss)
	# self.playMove(b, MCRate=r) should return a child of position b, chosen stochastically
	#   in some way so that better positions are more likely,
	#   a uniform random move is played with probability 'r'
	
	def __init__(self):
		pass
	
	def playGame(self, other, b=None, useMC=False, verbose=False):
		# play a full game between self and another player 'other'.
		# start from the board position 'b', or from a blank board if not specified.
		
		# 'self' plays as black, 'other' plays as white
		# return True if black wins.
		
		# if 'verbose', print out the game as it is played, at 1 move per second
		
		if b is None:
			b = newGame()
		
		i = 0
		while not b.isLost():
			player = self if b.blacksTurn else other # current player to play
			if useMC:
				# some of the time, play uniform random moves, otherwise play good moves
				b = player.playMove(b)
			else:
				# only play good moves
				b = player.playMove(b, MCRate=0)
			
			i += 1
			if verbose:
				# print the current board and wait 1 second so the user can watch
				print(b)
				sleep(1)
		
		if verbose:
			print("Number of moves: %i" % i)
		
		return not b.blacksTurn  # True if black won the game (the game was lost on white's turn)
	
	def MCEval(self, b, n=2000):
		# play out 'n' games starting at position 'b', in order to estimate the
		#   likelihood that the current player will win
		# return a number between -1 and 1
		# 1 if the current player always wins, -1 if they always lose
		
		wins = sum([ self.playGame(self,b=b) for k in range(n) ])
		return wins/n * 2 - 1   # range from -1 (white wins) to +1 (black wins)
	
	def playout(self, i, numEvals=2000):
		# play a full game. at every move, use MCEval to estimate the value (which involves
		#   playing out many subgames starting at that position)
		# return a list of the positions encountered in the main game,
		#   and a list of their estimated values, in the same order.
		# used to create training data
		
		print("Playout %i" % (i+1))
		
		b = newGame()
		positions = []
		values = []
		
		while not b.isLost():
			if b.isProceedable():
				b=b.proceed()
				# there is no need to recompute the value, as it is the same as before proceeding
			else:
				# play a move stochastically, and then compute the value of the resulting position
				b = self.playMove(b)
				v = self.MCEval(b, numEvals)
			
			if b.blacksTurn:
				c = b
				w = v
			else:
				# invert the board position so that the network sees black to move
				c = b.invert()
				w = -v
			
			# also include all symmetrically equivalent positions, using the same computed value
			for x in c.symmetries():
				positions.append(x)
				values.append(w)
		
		print("Playout %i complete" % (i+1))
		return positions, values
	
	def generateData(self, filename, numPlayouts=500, numEvals=1000, numjobs=60):
		# produce a collection of representative board positions by making some random playouts
		# estimate each postion's value by Monte Carlo playouts starting at that position
		# combine this data into two arrays, used for training
		#   X_train: the rows are unraveled representations of the board positions
		#   y_train: the computed values of the board positions
		# save the results to a file with name 'filename' (and also return them)
		
		# single-threaded implementation:
		# # results = [ self.playout(i, numEvals=numEvals) for i in range(numPlayouts) ]
		
		# parallel implementation, using 'numjobs' processes:
		results = Parallel(n_jobs=numjobs)( delayed(self.playout)\
			(i, numEvals=numEvals) for i in range(numPlayouts) )
		
		# collect together all the positions (and values) from each playout 
		#   into two big unnested lists
		positions, values = list(zip(*results))
		positions = sum(positions,[])
		values = sum(values,[])
		
		y_train = np.array( values )
		X_train = np.vstack( b.unravel() for b in positions )
		
		# save these arrays to a file
		with open( baseFilename + filename, 'wb' ) as f:
			pickle.dump( (X_train,y_train), f )
		
		return X_train, y_train
	
	def playBestMove(self, b):
	# play the best known move from position b, using self.eval to evaluate all possible moves
	
	# we make a list of pairs (v,m) with m a move and v its value
	# then sort it to find a best value, and make the corresponding move.
		moves = []
		for m in b.legalMoves():
			c = b.move(m)
			val = self.eval(c)
			
			if b.blacksTurn == c.blacksTurn:
				v = val
			else:
				v = -val
				# in this case, value after playing the move is value for our opponent,
				#   which is the negative of the value for us
			moves.append( (v, m) )
		# choose the move with the highest value for us
		moves.sort() 
		return b.move( moves[-1][1] ) 
	
	def scoreAgainst(self, other=None, numGames=100):
		# play 'numGames' games, with self as black and a player 'other' as white.
		# display the percentage of games won by self.
		# then repeat with colors switched.
		
		if other is None:
			other = randomPlayer()
		
		# play as black
		wins = sum( self.playGame(other) for i in range(numGames) )
		print('Winrate as black: %f' % (wins/numGames))
		
		# play as white
		losses = sum( other.playGame(self) for i in range(numGames) )
		print('Winrate as white: %f' % (1 - losses/numGames))
	
	def gameLength(self, numGames=10):
		# play out a few games to estimate how long games usually last when self plays self
		
		def length(t):
			# play a game and return how many moves it lasted for.
			i = 0
			b = newGame()
			while not b.isLost():
				b = t.playMove(b)
				i += 1
			return i
		
		# return the average game length
		return sum([length(self) for i in range(numGames)]) / numGames
	
	def testingBoard1(self):
		# a test to see how intelligent self is.
		# this is a board position where, no matter which move is played, the
		#   game will end right away
		# four moves result in a win, the rest result in loss
		# we query self.eval on this position and all of its children
		#    to see how well self can distinguish the winning moves from the losing ones
		
		b=moveBoard(bpieces=4,wpieces=4)
		b=b.move(('d',0)).proceed().proceed()
		b=b.move(('u',0)).proceed().proceed()
		b=b.move(('d',0)).proceed().proceed()
		b=b.move(('u',0)).proceed().proceed()
		b=b.move(('d',0)).proceed().proceed()
		b=b.move(('d',5)).proceed().proceed()
		
		print(b)
		r=randomPlayer()

		print( 'Random MCEval: %f, \t Network eval: %f, \t Network MCEval: %f \n' % \
			(r.MCEval(b), self.eval(b), self.MCEval(b)) )
		
		for m in b.legalMoves():
			# for each possible move, print the value (always +1 or -1) and the network's value
			bb = b.move(m)
			print( 'Random MCEval: %f, \t Network eval: %f' % (r.MCEval(bb), self.eval(bb)) )
		print('\n')


class randomPlayer(player):
	# a player who plays uniform random moves
	
	def playMove(self, b, MCRate=0):
		# play a uniform random legal move
		return b.randMove()
	
	def eval(self, b):
		# evaluate by performing uniform random playouts
		return self.MCEval(b)


class networkPlayer(player):
	# a player driven by a neural network
	
	# networkPlayer() will create a new player with an untrained neural net
	# networkPlayer(i) will load in the previously saved neural network from training iteration 'i'
	
	def __init__(self, filename=None):
		super().__init__()
		
		if filename is not None:
			# load in the previously trained neural net
			with open(baseFilename + str(filename), 'rb') as f:
				self.mlp = pickle.load(f)
		else:
			# initialize a new neural net (scikit-learn MLPRegressor)
			self.mlp = MLPRegressor(activation='relu', hidden_layer_sizes=(100,100,100,100,100,100), \
				alpha=0.0, tol=1e-5, learning_rate_init=0.001)
			# we set alpha=0 because we have observed no significant overfitting
			#   occurring with our current training technique
			# that is, the score against test data does not begin to decrease significantly
			
			# keep track of how many training iterations have occurred
			# one iteration means:
			#   (1) collect a block of training data, using the current network
			#   (2) train the network several times on the same data, updating the network
			self.mlp.iters = 0
	
	def train(self, Xytrain, Xytest=None, numsteps=10, save=True):
		# perform training on the neural network, using previously generated training data 'Xytrain'
		
		# if 'Xytest' is passed in, it is used as test data, to display the performance
		#   of the network during training.  This data is not used for training, 
		#   just to monitor convergence
		# 'numsteps' is the number of optimization steps to perform on the same data
		# if 'save', then we save the trained network to a file named by the iteration number
		#   (self.mlp.iters) after the training.
		
		print('Training iteration number %i\n' % self.mlp.iters)
		
		X_train, y_train = Xytrain
		
		if self.mlp.iters>0:
			# display the network's score against the new data before training, as a baseline
			print("Pretraining score: %f" % self.mlp.score(X_train[:80000,:], y_train[:80000]))
			if not (Xytest is None):
				print("Pretraining test set score: %f" % self.mlp.score(*Xytest))
		
		for i in range(numsteps):
			# update the neural network (one optimization step)
			self.mlp.partial_fit( X_train, y_train )
			
			# display the accuracy score of the network against training and test sets
			#   score of 1: perfect accuracy,
			#   score of 0: same performance as a constant guess at the mean value
			print("Training set score (step %i): %f" % \
				(i, self.mlp.score(X_train[:80000,:], y_train[:80000])))
			if not (Xytest is None):
				print("Test set score (step %i): %f" % (i, self.mlp.score(*Xytest)))
		
		# increment the number of iterations, and save the trained model to a file
		self.mlp.iters += 1
		print('\n',)
		if save:
			self.save(str(self.mlp.iters))
		return self
	
	def save(self, filename):
		# save a trained neural network to a file so that it can be loaded back in later
		
		with open(baseFilename + filename, 'wb') as f:
			pickle.dump(self.mlp, f)
		
		# note: this is probably quite wasteful in terms of file size; it would be better to extract 
		#   only the coefficient matrices and other meaningful data from self.mlp.
		# but this is flexible and safe, so good enough for now
	
	def eval(self, b):
		# predict the value of the position 'b' for the current player
		# returns a single float
		
		# behavior is neural-network based and unfiltered, so no guarantees.
		# expected behavior:
		#   values between -1 and +1
		#   a return value of +1 means the player to move will win for certain, 
		#   a -1 means the player to move will lose for certain
		
		if not b.blacksTurn:
			# the neural network can only handle a position in which it's black's turn, and
			#   will return an estimate of the likelihood of black to win.
			# so if it's white's turn, we need to invert the colors of the board before
			#   asking the network.  (but the value does not need inversion)
			b = b.invert()
		
		# feed the position 'b' into the neural network.
		# the 'reshape(1,-1)' is needed because the network expects to evaluate a bunch of positions
		#   at once, so needs to see a 2-D array (with just one row)
		# the '[0]' is needed because the output is a vector with just one component
		return self.mlp.predict(b.unravel().reshape(1,-1))[0]
	
	def playMove(self, b, MCRate=0.2, pickiness=100.0):
		# play a move from board position 'b'
		# this is used for generating training data.
		# the MCRate parameter is used to ensure that the resulting playouts explore the
		#   game tree widely, and have a chance to discover good options that the network rates poorly
		
		# with probability MCRate, play uniform-randomly
		# with probability 1-MCRate, use the neural network:
		# accept or reject a move based on
		#   how much it decreases the estimated board value.
		#   pickiness = infty: no decrease allowed.
		#   pickiness = 0: uniform random play.
		
		# note: it should be better to accept/reject moves based on how they compare to the
		#   best possible move, rather than how they compare to the current position.
		# but this makes training much slower.
		
		if random.uniform(0,1) < MCRate:
			# just play a uniform random move
			return b.randMove()
		
		# evaluate the current position, or use the precalculated evaluation
		try:
			curval = b.cachedValue(self)
		except ValueError:
			curval = self.eval(b)
		
		# choose the first move we find which is acceptable.
		# accept move which increase the value always, 
		# accept moves which decrease the value with some probability depending on the decrease
		legals = b.legalMoves()
		random.shuffle(legals)
		for m in legals:
			c = b.move(m)
			val = self.eval(c)
			
			if b.blacksTurn == c.blacksTurn:
				v = val
			else:
				# in this case, value after playing the move is value for our opponent
				v = -val
			
			# accept this move if it increases the value estimate,
			#   or (stochastically) if it decreases it only slightly
			if (v > curval) or ( random.uniform(0,1) < (1-(curval-v)/2.0)**pickiness ):
				# accept this move, and cache the evaluation we just computed
				c.cacheValue(self, val)
				return c
		
		# we didn't accept anything.  Just play at random.
		return c

def loadData(filename):
	# load in previously computed training/test data from a file
	# return two arrays: X_values, y_values
	# the returned 2-tuple is used as 'Xytrain' or 'Xytest'
	#   in a call to networkPlayer().train
	
	with open( baseFilename + filename, 'rb' ) as f:
		d = pickle.load(f)
	return d



###############################################################################
# Gipf board position setup
###############################################################################

# setup global constants describing the board for the game GIPF,
#   including the physical shape of the board, the directions in which
#   pieces can slide, and the board symmetries

# the board will be represented as an array with some unused locations:
# ...XXXX
# ..XXXXX
# .XXXXXX
# XXXXXXX
# XXXXXX.
# XXXXX..
# XXXX...

# a mask representing the hexagonal board shape.  The False entries mark
#   locations which are not valid spots on the board, and will never be used.
valid_spots = np.array( [[ (2<i+j<10) for i in range(7) ] for j in range(7) ] )

# vectors representing the six directions that pieces can slide in:
#   right (r), up-right (ur), up (u), left (l), down-left (dl), down (d)
directions = { 'r':np.array([0,1]), 'ur':np.array([-1,1]), 'u':np.array([-1,0]), \
	'l':np.array([0,-1]), 'dl':np.array([1,-1]), 'd':np.array([1,0]) }

# a move in the game will be represented by a direction and a parallel (d,i), e.g.
#  ('r',4) means sliding rightwards along the 5th parallel horizontal row,
#   counted counter-clockwise:

# ...XXXX
# ..XXXXX
# .XXXXXX
# XXXXXXX
# >>>>>>.
# XXXXX..
# XXXX...

# slides: list of all possible slides (d,i), for d a direction and 0 <= i < 7.
# slidingStart[(d,i)]: the (x,y) coordinates of the starting position for the slide (d,i)
slidingStart = {}
for i in range(7):
	slidingStart[('r',i)]  = np.array([i,max(3-i,0)])
	slidingStart[('ur',i)] = np.array([min(i+3,6),max(i-3,0)])
	slidingStart[('u',i)]  = np.array([min(6,9-i),i])
	slidingStart[('l',i)]  = np.array([6-i,min(6,3+i)])
	slidingStart[('dl',i)] = np.array([max(0,3-i),min(6,9-i)])
	slidingStart[('d',i)]  = np.array([max(0,i-3),6-i])
slides = list(slidingStart)

# slidingLength[i]: how many spots long the slide (d,i) is
slidingLength = [4,5,6,7,6,5,4]

# sliding_paths[(d,i)]: list of board locations along the slide (d,i)
sliding_paths = {}
for (d,i) in slidingStart:
	sliding_paths[(d,i)] = [ slidingStart[(d,i)] + t*directions[d] \
		for t in range(slidingLength[i]) ]

def rotateMatrix(b):
	# takes a matrix representing a board state and rotates it 60 degrees counterclockwise
	
	newb=np.zeros([7,7],dtype=np.bool_)

	newb[0,3:7]=np.transpose(b[0:4,6])
	newb[1,2:7]=np.transpose(b[0:5,5])
	newb[2,1:7]=np.transpose(b[0:6,4])
	newb[3,:]=np.transpose(b[:,3])
	newb[4,0:6]=np.transpose(b[1:7,2])
	newb[5,0:5]=np.transpose(b[2:7,1])
	newb[6,0:4]=np.transpose(b[3:7,0])
	return newb


# classes representing board positions:
#   board: base class
#   moveBoard: a board position in which a sliding move can be made
#   captureBoard: a board position in which a capture move can be made

# in Gipf, after a sliding move has been made, both players may have the opportunity to 
#   make captures, as many times as possible.
# they may choose the order of captures, if more than one capture is possible.
# So, we model each move in three phases:
# (1) precapture phase: make captures in any order you wish, until no longer possible
# (2) move phase: make a sliding move
# (3) postcapture phase: make captures in any order you wish, until no longer possible

# a "move phase" is represented by a moveBoard
# a "precapture phase" is represented by a captureBoard with precapturePhase=True
# a "postcapture phase" is represented by a captureBoard with precapturePhase=False

# when a move is made from any of these types of board, it will return a
#   newly created board of the appropriate type


class board(object):
	# base class for Gipf game board positions
	# not to be instanced directly.  Use moveBoard() to create a blank game board.
	# represents an immutable board position.  Making moves or other changes to the position
	#   should create a new instance
	
	# interface:
	#   self.blacksTurn: True if it is black's turn, False if it white's turn
	#   self.isLost(): True if the game is over (and so the current player loses)
	#   self.isProceedable(): True if the move is forced (only one option is possible)
	#   self.proceed(): play the forced move, producing a new board position
	#   self.legalMoves(): list of legal moves in this board position
	#   self.move(m): make the move m (which is assumed to be legal), producing a new board position
	#   self.randMove(): make a uniform random legal move, producing a new board position
	#   self.unravel(): representation of the board position as a 1-D numpy array
	#     (entries should each have roughly mean 0 and std 1)
	#   self.cacheValue(p,v): store the value 'v' of this position as estimated by player 'p'
	#   self.cachedValue(p): retrieve the value of this position computed by player 'p'
	#     or raise ValueError, if not possible
	
	def __init__(self, blacksTurn=True, bpieces=15, wpieces=15, b=None, w=None):
		# blacksTurn: True if it is black's turn, False if it is white's turn
		# bpieces: the number of pieces black has in reserve
		# wpieces: the number of pieces white has in reserve
		
		self.blacksTurn=blacksTurn
		self.bpieces=bpieces
		self.wpieces=wpieces
		
		# initialize the board position with the matrices  b and w, or else with all zeros
		if b is None:
			self.black = np.zeros([7,7], dtype=np.bool_)
		else:
			self.black = b
		if w is None:
			self.white = np.zeros([7,7], dtype=np.bool_)
		else:
			self.white = w
		
		# slots for caching computed values of the board position to avoid repeatedly
		#   computing them when passing boards around
		self.legalMoveList = None
		self._cachedValue = None
	
	def __str__(self):
		# printable string description of the board position
		p=''
		for i in range(7):
			for j in range(7):
				if not valid_spots[i,j]:
					p+='  '
				elif self.black[i,j]:
					p+='b '
				elif self.white[i,j]:
					p+='w '
				else:
					p+='. '
			p+='\n'
		p+='Black pieces: %i, White pieces: %i\n' % (self.bpieces, self.wpieces)
		p+='Turn: ' + ('B\n' if self.blacksTurn else 'W\n')
		return p
	
	def symmetries(self):
		# list of board positions equivalent to self under the board symmetries:
		#   six rotations, six reflections
		l = []
		b = self
		for k in range(6):
			l += ( b, b.reflect() )
			b = b.rotate()
		return l
	
	def unravel(self):
		# represent the board position as a single one-dimensional array
		# used as input to the neural network
		# shift and scale the array so that in a typical collection of board positions
		#  each entry has roughly mean 0 and standard deviation 1
		
		mean, std = .2, .35
		
		# the last two entries in the array represent whether this is a captureBoard or a moveBoard,
		#   and, in the second case, whether it is precapture or postcapture phase
		if isinstance(self, captureBoard):
			cap = 1
			precap = self.precapturephase
		else:
			cap = 0
			precap = 0
		
		m = np.hstack([ np.extract(valid_spots, self.black), \
			np.extract(valid_spots, self.white), \
			[ self.bpieces/15.0, self.wpieces/15.0, self.blacksTurn, cap, precap ] ])
		return (m-mean)/std
	
	def cacheValue(self, player, value):
		# the player "player" has estimated the value of this board position to be "value".
		#   save this information, so that later if the same player tries to compute
		#   it again, it can retrieve the answer using self.cachedValue(player)
		self._cachedValue = (player, player.mlp.iters, value)
	
	def cachedValue(self, player):
		# retrieve the value of this position previously computed by player.eval(self)
		#   if it doesn't exist, raise ValueError
		# if the value was previously computed using a different player or using the same
		# player on a previous training iteration, then we should reject it and recompute the value
		
		if self._cachedValue is None:
			raise(ValueError)
		if (player, player.mlp.iters) == self._cachedValue[:1]:
			return self._cachedValue[2]
		else:
			raise(ValueError)


class captureBoard(board):
	# board class used to represent a board position in the precapture or postcapture phase
	
	def __init__(self, blacksTurn=True, precapturephase=False, 
		bpieces=15, wpieces=15, b=None, w=None):
		super().__init__(blacksTurn, bpieces, wpieces, b, w)
		self.precapturephase = precapturephase
	
	def __str__(self):
		# add a line to the end saying whether this is a pre- or post-move capture phase
		return super().__str__() + ('Premove capture phase\n' if self.precapturephase 
			else 'Postmove capture phase\n')
	
	def proceed(self):
		# called in a capture phase when no capture is possible.
		# it converts self to a new board, in the next phase in the sequence
		#   precapture phase -> move phase -> postcapture phase -> precapture phase -> ....
		
		# we should assert( self.isProceedable() ) here, 
		#   but we don't check, for efficiency of training
		
		if self.precapturephase:
			return moveBoard( blacksTurn=self.blacksTurn, \
				bpieces=self.bpieces, wpieces=self.wpieces, b=self.black, w=self.white )
		else:
			return captureBoard( blacksTurn=not self.blacksTurn, precapturephase=True, \
				bpieces=self.bpieces, wpieces=self.wpieces, b=self.black, w=self.white )
	
	def randMove(self):
		# play a uniform random capture move (or proceed if no capture is possible)
		
		m = random.choice(self.legalMoves())
		return self.move(m)
	
	def legalMoves(self):
		# list the possible capture moves
		
		# if the list of legal moves in the board position have been computed before,
		#   use them instead of recomputing
		if self.legalMoveList is not None:
			return self.legalMoveList
		
		if self.blacksTurn:
			cur = self.black
		else:
			cur = self.white
		
		# detect all possible rows (d,i) which have possible captures
		#   that is, clumps of at least four pieces of our color in a row
		# only use three directions 'r', 'ur', 'u'.  Since captures don't have a 
		#   directionality like slides do, there is no difference between e.g. up and down
		
		# The code below is equivalent to the following:
		# # for d in ('r','ur','u'):
		# # 	for i in range(7):
		# # 		path = sliding_paths[d,i]
		# # 		row = [ cur[p[0],p[1]] for p in path ]
		# # 		for j in range(slidingLength[i]-3):
		# # 			if all( row[j:j+4] ):
		# # 				l.append((d,i))
		# # 				break
		
		# as a shortcut, we only look for capturable strings of pieces for those (d,i)
		#   where there is a piece of our color on the fourth spot along the corresponding
		#   sliding path.  This saves a lot of expensive checking, and it has to be the case
		#   if there are four in a row (since rows are at most 7 spots long)
		
		l = []
		for i in range(7):
			if cur[i,3]:
				if self.isLegal(('r',i)):
					l.append(('r',i))
				if self.isLegal(('ur',i)):
					l.append(('ur',i))
			if cur[3,i]:
				if self.isLegal(('u',i)):
					l.append(('u',i))
		# if no possible captures were identified, just list a 
		#   legal move for ending this capture phase
		if len(l) == 0:
			l.append(('continue',0))
		
		# cache the computed list of legal moves in the board position for later reuse
		self.legalMoveList = l
		return l
	
	def isProceedable(self):
		# return True if no capture is possible, False if a capture is possible
		
		return ('continue',0) in self.legalMoves()
	
	def isLegal(self, m):
		# return True if a capture is possible along row m=(d,i)
		
		d,i = m
		
		path = sliding_paths[d,i]
		if self.blacksTurn:
			cur = self.black
		else:
			cur = self.white
		
		# are there four in a row of our color?
		row = [ cur[p[0],p[1]] for p in path ]
		for j in range(slidingLength[i]-3):
			if all( row[j:j+4] ):
				return True
		return False
		
	def move(self, m):
		# perform the capture along row m=(d, i)
		# we should be first checking that this capture is possible:
		#   assert(m in self.legalMoves())
		# but we don't check, for training efficiency
		
		d,i = m
		
		# if there's no capture possible, just proceed to the next phase
		if d == 'continue':
			return self.proceed()
		
		# we need to locate and remove the "clump" of pieces along row (d,i). 
		#   That is, the largest contiguous row of pieces which contains
		#   at least four in a row of our color.
		# For example, if it's black's turn,
		#   w.wbbbb
		#   becomes
		#   w......
		
		p = sliding_paths[d,i]
		middle = slidingLength[i]-4  # a capture clump is guaranteed to contain this spot
		
		newb = self.black.copy()
		neww = self.white.copy()
		bcaps = 0
		wcaps = 0
		
		# search from middle downward, removing all pieces encountered of both colors
		#   until we hit a blank spot
		# count the pieces that get removed, of both colors
		for k in range(middle,-1,-1):
			if self.black[ p[k][0],p[k][1] ]:
				bcaps += 1
				newb[  p[k][0],p[k][1] ] = False
				#assert( not self.white[ p[k][0],p[k][1] ] )
			elif self.white[ p[k][0],p[k][1] ]:
				wcaps += 1
				neww[  p[k][0],p[k][1] ] = False
				#assert( not self.black[ p[k][0],p[k][1] ] )
			else:
				# found a blank spot, stop searching downward
				break
		
		# search from middle upwards (as before)
		for k in range(middle+1,middle+4):
			if self.black[ p[k][0],p[k][1] ]:
				bcaps += 1
				newb[  p[k][0],p[k][1] ] = False
				#assert( not self.white[ p[k][0],p[k][1] ] )
			elif self.white[ p[k][0],p[k][1] ]:
				wcaps += 1
				neww[  p[k][0],p[k][1] ] = False
				#assert( not self.black[ p[k][0],p[k][1] ] )
			else:
				# found a blank spot, stop searching upward
				break
		
		# our pieces that got removed are returned to us, so add them to the number
		#   of availiable pieces
		# our oppenent's pieces that got removed are gone forever.
		if self.blacksTurn:
			newbpieces = self.bpieces + bcaps
			newwpieces = self.wpieces
		else:
			newbpieces = self.bpieces
			newwpieces = self.wpieces + wcaps
		
		# return a new captureBoard of the same phase
		return captureBoard(blacksTurn=self.blacksTurn, precapturephase=self.precapturephase, \
			bpieces=newbpieces, wpieces=newwpieces, b=newb, w=neww)
	
	def isLost(self):
		# it is not possible for the game to end on a capture phase.
		return False
		
	def rotate(self):
		# rotate the board 60 degrees counter-clockwise (producing a new captureBoard)
		newb, neww = rotateMatrix(self.black), rotateMatrix(self.white)
		
		return captureBoard(blacksTurn=self.blacksTurn, \
			precapturephase=self.precapturephase, bpieces=self.bpieces, \
			wpieces=self.wpieces, b=newb, w=neww)
	
	def reflect(self):
		# reflect the board along the diagonal (producing a new captureBoard)
		return captureBoard(blacksTurn=self.blacksTurn, precapturephase=self.precapturephase, \
			bpieces=self.bpieces, wpieces=self.wpieces,  \
			b=np.transpose(self.black), w=np.transpose(self.white))
	
	def invert(self):
		# swap black and white (producing a new captureBoard)
		return captureBoard(blacksTurn=not self.blacksTurn, precapturephase=self.precapturephase, \
			bpieces=self.wpieces, wpieces=self.bpieces, b=self.white,w=self.black)


class moveBoard(board):
	# board class used to represent a board position in the move phase
	# that is, the active player will make a sliding move 
	
	def __str__(self):
		# add a line to the end of the string representation, saying that this is a move phase
		return super().__str__() + 'Move phase\n'
	
	def isLegal(self, m):
		# return True if it is possible to make a sliding move along row m=(d,i)
		# it is not possible if the whole row is already filled up with pieces
		
		d,i = m
		p = sliding_paths[d, i]
		return not all( self.black[s[0],s[1]] or self.white[s[0],s[1]] for s in p )
	
	def randMove(self):
		# make a uniform random sliding move
		# we could do this:
		# # d,i = random.choice(self.legalMoves())
		# but, instead of choosing a legal sliding move uniformly, we just choose
		#   a sliding row (d,i) at random.  Discard those that end up being illegal moves.
		# this is equivalent, and saves time since most moves are legal most of the time
		
		while True:
			m = random.choice(slides)
			if self.isLegal(m):
				break
		# it is not possible that all moves are illegal, since there are not enough
		#   pieces in the game to fill the board completely.  So this while loop will terminate
		
		return self.move(m)

	def move(self, m):
		# perform the sliding move along row m=(d,i), ending the current move phase and producing
		#  a new captureBoard in the post-move capture phase.
		# slide the row m=(d,i).  That is, move each stone one place along the row until the first
		#   empty spot is encountered.
		# place a new stone of our color at the start of the row.
		
		# we should really check if a sliding move along row m=(d,i) is legal before doing it:
		# # assert( self.isLegal(m) )
		# but we don't check, for training efficiency
		
		d,i = m
		
		p = sliding_paths[d, i]
		newb = self.black.copy()
		neww = self.white.copy()
		
		for i in range(slidingLength[i] - 1):
			if not self.black[ p[i][0],p[i][1] ] and not \
				self.white[ p[i][0],p[i][1] ]:
				# we've reached the end of the clump of pieces to slide.
				break
			# slide the piece one spot along the row
			newb[ p[i+1][0],p[i+1][1] ] = self.black[ p[i][0],p[i][1] ]
			neww[ p[i+1][0],p[i+1][1] ] = self.white[ p[i][0],p[i][1] ]	
		
		# place a new piece of our color at the start of the sliding row
		if self.blacksTurn:
			newb[ p[0][0],p[0][1] ]=True
			neww[ p[0][0],p[0][1] ]=False
			bpieces=self.bpieces-1
			wpieces=self.wpieces
		else:
			newb[ p[0][0],p[0][1] ]=False
			neww[ p[0][0],p[0][1] ]=True
			bpieces=self.bpieces
			wpieces=self.wpieces-1
		
		# after the move, we enter a post-move capture phase.
		return captureBoard(blacksTurn=self.blacksTurn, precapturephase=False, \
			bpieces=bpieces, wpieces=wpieces, b=newb, w=neww)
	
	def legalMoves(self):
		# list the sliding rows (d,i) which we can slide along (i.e. the ones that aren't full)
		
		# use the cached value if it has been computed previously
		if self.legalMoveList is not None:
			return self.legalMoveList
		
		# otherwise, compute the list of all legal moves, and cache it as self.legalMoveList
		self.legalMoveList = [ m for m in slidingStart if self.isLegal(m) ]
		return self.legalMoveList
	
	def isLost(self):
		# return True if the game is over
		# that is, if the current player has no pieces left, so can't make a sliding move
		
		return (self.blacksTurn and not self.bpieces) or (not self.blacksTurn and not self.wpieces)
	
	def invert(self):
		# swap black and white, producing a new moveBoard
		
		return moveBoard( blacksTurn=not self.blacksTurn, \
			bpieces=self.wpieces, wpieces=self.bpieces, b=self.white, w=self.black )
	
	def rotate(self):
		# rotate the board 60 degrees counter-clockwise, producing a new moveBoard
		
		b,w = rotateMatrix(self.black), rotateMatrix(self.white)
		return moveBoard( blacksTurn=self.blacksTurn, bpieces=self.bpieces, wpieces=self.wpieces, \
			b=b, w=w )
	
	def reflect(self):
		# reflect the board along the diagonal, producing a new moveBoard
		return moveBoard( blacksTurn=self.blacksTurn, bpieces=self.bpieces, wpieces=self.wpieces, \
			b=np.transpose(self.black),w=np.transpose(self.white))
	
	def isProceedable(self):
		# this is only possible for captureBoards.
		# moveBoards can never proceed; only sliding moves are allowed.
		
		return False


# create a singleton object for a blank game board
# this prevents calculating its value repeatedly,
# and allows any extensions to easily specify what the beginning of their game is
blankBoard = moveBoard()
def newGame():
	return blankBoard








